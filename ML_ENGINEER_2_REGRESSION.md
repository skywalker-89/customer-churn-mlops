# ML Engineer #2: Regression Model - Task Assignment

**Your Mission**: Implement a regression model to predict revenue per session

**Timeline**: 1-3 weeks  
**Performance Target**: RMSE < $20, MAE < $15, R¬≤ > 0.50

---

## üéØ Your Objective

Predict the revenue generated by a website session.

**Target Variable**: `revenue` (continuous: $0 - $150, most orders ~$60)

**Challenge**: **Zero-inflation** - 93.2% of sessions have $0 revenue

---

## üìÅ Your File

**Location**: [src/regression/train_model.py](file:///Users/jul/Desktop/uni/customer-churn-mlops/src/regression/train_model.py)

**What's Already Done**:
- ‚úÖ Data loading from MinIO
- ‚úÖ Strategy selection (`all` vs `converting_only`)
- ‚úÖ Train/test split (80/20)
- ‚úÖ MLflow experiment tracking
- ‚úÖ Model evaluation (RMSE, MAE, R¬≤, MAPE)
- ‚úÖ Model saving to MinIO

**What YOU Need to Do**:
- ‚ö†Ô∏è Implement the `train_model()` function (line 93-155)

---

## üîß Step-by-Step Instructions

### Step 1: Open Your File

```bash
cd /Users/jul/Desktop/uni/customer-churn-mlops
code src/regression/train_model.py
```

### Step 2: Find the Function to Implement

**Line 93-155**: Look for this function:

```python
def train_model(self, X_train, y_train):
    """
    üö® ML ENGINEERS: IMPLEMENT YOUR MODEL HERE
    ...
    """
    
    # PLACEHOLDER - Replace with your implementation
    print("\n‚ö†Ô∏è  PLACEHOLDER MODEL - Replace with actual implementation")
    from sklearn.dummy import DummyRegressor
    model = DummyRegressor(strategy='mean')
    model.fit(X_train, y_train)
    
    return model
```

**Your Task**: Replace the `DummyRegressor` with a real model.

---

## üìä The Data

**Training Data**: `s3://processed-data/training_data.parquet`

**Features** (X_train has 12 features):
- `is_repeat_session` (binary)
- `hour_of_day` (0-23)
- `is_weekend` (binary)
- `engagement_depth` (number of pages viewed) ‚≠ê **STRONGEST PREDICTOR**
- `utm_source_*` (one-hot encoded)
- `device_type_mobile` (one-hot encoded)
- `landing_page_*` (one-hot encoded)

**Target** (y_train):
- `revenue`: $0 to ~$150 (most common: $59.99)

**Distribution**:
- Training set: Varies based on strategy (see below)
- Test set: Same
- Revenue stats:
  - Mean: $4.10 per session (most are $0)
  - AOV (Average Order Value): $59.99
  - 93.2% have $0 revenue

---

## üö® CRITICAL: Choose Your Strategy

You have **3 options** for handling zero-inflation:

### Strategy 1: Regression on Converting Sessions Only (RECOMMENDED)

**Approach**: Only train on sessions where `is_ordered = 1` (6.8% of data)

**Pros**:
- ‚úÖ Simpler problem (no zeros)
- ‚úÖ Higher quality predictions
- ‚úÖ Easier to interpret (predicting order value, not session value)

**Cons**:
- ‚ùå Smaller training set (~32K samples instead of 472K)
- ‚ùå Doesn't predict if user will convert (separate classification step needed)

**Implementation**:
```python
# In __main__ at bottom of file (line 245):
trainer = RegressionModelTrainer(strategy='converting_only')
trainer.run()
```

**When to Use**: Always start here - it's the simplest and most effective

---

### Strategy 2: Two-Stage Model (ADVANCED)

**Approach**: 
1. Stage 1: Classification model predicts if user converts
2. Stage 2: Regression model (your model) predicts revenue for predicted converters

**Pros**:
- ‚úÖ Separates concerns (conversion vs. amount)
- ‚úÖ Handles zero-inflation naturally
- ‚úÖ Production-ready approach

**Cons**:
- ‚ùå More complex (requires coordination with classification engineer)
- ‚ùå Errors compound (bad classification ‚Üí bad regression)

**Implementation**:
```python
# Step 1: Load classification model
classification_model = joblib.load('models/classification_model.pkl')

# Step 2: Predict who will convert
conversion_predictions = classification_model.predict(X_test)

# Step 3: Predict revenue only for predicted converters
revenue_pred = np.zeros(len(X_test))
converter_mask = (conversion_predictions == 1)
revenue_pred[converter_mask] = your_regression_model.predict(X_test[converter_mask])
```

**When to Use**: After both models are working individually

---

### Strategy 3: Direct Regression on All Sessions

**Approach**: Predict revenue for all sessions (including $0)

**Pros**:
- ‚úÖ Single model (simpler infrastructure)
- ‚úÖ Uses all data

**Cons**:
- ‚ùå Model learns to predict ~$4 for everything (useless)
- ‚ùå Zero-inflation dominates signal
- ‚ùå Poor performance on non-zero predictions

**Recommendation**: **Avoid** - this doesn't work well in practice

---

## ‚úÖ Recommended Implementation (Strategy 1)

### Baseline: Random Forest on Converting Sessions

```python
def train_model(self, X_train, y_train):
    from sklearn.ensemble import RandomForestRegressor
    
    print("   Training Random Forest Regressor...")
    print(f"   Training on {len(X_train)} converting sessions")
    print(f"   Revenue range: ${y_train.min():.2f} - ${y_train.max():.2f}")
    print(f"   Mean revenue: ${y_train.mean():.2f}")
    
    model = RandomForestRegressor(
        n_estimators=200,
        max_depth=20,
        min_samples_split=10,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    return model
```

### Advanced: XGBoost with Log Transform

```python
def train_model(self, X_train, y_train):
    import xgboost as xgb
    import numpy as np
    
    # Log transform to handle long tail
    print("   Applying log transformation to target...")
    y_log = np.log1p(y_train)  # log(1 + y) to handle edge cases
    
    print("   Training XGBoost Regressor...")
    model = xgb.XGBRegressor(
        max_depth=8,
        learning_rate=0.05,
        n_estimators=300,
        random_state=42
    )
    model.fit(X_train, y_log)
    
    # Wrap model to inverse transform predictions
    class LogTransformWrapper:
        def __init__(self, model):
            self.model = model
        
        def predict(self, X):
            y_log_pred = self.model.predict(X)
            return np.expm1(y_log_pred)  # exp(y) - 1
        
        def __getattr__(self, attr):
            return getattr(self.model, attr)
    
    return LogTransformWrapper(model)
```

### Ultra-Advanced: Quantile Regression

```python
def train_model(self, X_train, y_train):
    from sklearn.ensemble import GradientBoostingRegressor
    
    # Predict median instead of mean (more robust to outliers)
    print("   Training Quantile Regression...")
    model = GradientBoostingRegressor(
        loss='quantile',
        alpha=0.5,  # Predict median
        n_estimators=200,
        max_depth=10,
        learning_rate=0.1,
        random_state=42
    )
    model.fit(X_train, y_train)
    
    return model
```

---

## ‚úÖ Testing Your Model

### Test Locally (Before Airflow)

```bash
# Run your model
python src/regression/train_model.py
```

**Expected Output**:
```
============================================================
  REGRESSION MODEL TRAINING
  Task: Revenue Prediction
============================================================
üì• Loading training data...
   Strategy: Converting sessions only
‚úÖ Loaded 32,313 samples, 12 columns
   Features: 12
   Target stats: mean=$59.99, std=$10.50, max=$99.99

üîÄ Splitting data (test_size=0.2)...
   Train: 25,850 samples
   Test:  6,463 samples

   Training Random Forest Regressor...

üìä Evaluating model...

üéØ Regression Metrics:
   RMSE:  $18.50
   MAE:   $12.30
   R¬≤:    0.55
   MAPE:  18.50%

üíæ Saving model to MinIO...
‚úÖ Model saved to s3://models/regression_model.pkl

============================================================
‚úÖ TRAINING COMPLETE!
   MLflow Run ID: xyz789...
   RMSE: $18.50
   MAE:  $12.30
   R¬≤:   0.55
============================================================
```

### Check MLflow

```bash
# Open MLflow UI
open http://localhost:5001
```

Navigate to:
- Experiment: `Revenue_Prediction_Regression`
- Check your run's metrics:
  - `rmse`
  - `mae`
  - `r2`
  - `mape`

### Test in Airflow

```bash
# Trigger the model training DAG
airflow dags trigger model_training_pipeline
```

Check Airflow UI at `http://localhost:8080`:
- DAG: `model_training_pipeline`
- Task: `train_regression_model`
- View logs

---

## üéØ Performance Targets

| Metric | Minimum Target | Good Target | Notes |
|--------|---------------|-------------|-------|
| **RMSE** | < $25 | < $20 | Root Mean Squared Error (penalizes large errors) |
| **MAE** | < $18 | < $15 | Mean Absolute Error (average error magnitude) |
| **R¬≤** | > 0.40 | > 0.50 | Explained variance (how much better than mean) |
| **MAPE** | < 35% | < 25% | Mean Absolute Percentage Error |

**Context**: Average Order Value (AOV) = $59.99

---

## üìö Algorithm Recommendations

### Phase 1: Baseline (Start Here)
```python
# Random Forest on converting sessions only
# Expected RMSE: $22-25
# Expected R¬≤: 0.40-0.50
```

### Phase 2: Advanced
```python
# XGBoost with log transform
# Expected RMSE: $18-22
# Expected R¬≤: 0.50-0.60
```

### Phase 3: Ensemble
```python
# Combine Random Forest + XGBoost
# Expected RMSE: $15-18
# Expected R¬≤: 0.60-0.70
```

---

## üîç Key Insights from EDA

**Use these to guide your modeling**:

1. **Revenue Distribution**
   - Most orders: $50-$70 (tight distribution)
   - AOV: $59.99
   - Very few outliers > $100
   - **Insight**: Fixed-price products or limited SKU

2. **Engagement Matters for Revenue Too**
   - Higher engagement ‚Üí higher likelihood of higher-value orders?
   - Check correlation in your model

3. **Repeat Users Spend More?**
   - Check if `is_repeat_session` correlates with revenue
   - May indicate customer loyalty

**Read More**: [FEATURE_ANALYSIS_STRATEGY.md](file:///Users/jul/Desktop/uni/customer-churn-mlops/FEATURE_ANALYSIS_STRATEGY.md)

---

## üõ†Ô∏è Troubleshooting

### Issue: RMSE = $40+ (Very High)
**Cause**: Model can't capture variance (predicting mean for everything)  
**Fix**: Try log transform, add more features, or use ensemble methods

### Issue: R¬≤ = Negative
**Cause**: Model performs worse than just predicting the mean  
**Fix**: Check for data leakage, ensure proper train/test split, try simpler model

### Issue: RMSE < $10 (Too Good to Be True)
**Cause**: Data leakage (target info leaked into features)  
**Fix**: Check features - remove anything calculated AFTER the order

### Issue: Predictions All the Same
**Cause**: Model collapsed to predicting mean/median  
**Fix**: Increase model complexity, add regularization, try different algorithm

### Issue: MLflow Logging Fails
**Cause**: MLflow server not running  
**Fix**: Start MLflow server
```bash
mlflow server --host 0.0.0.0 --port 5001
```

---

## üß™ Advanced Techniques (Optional)

### 1. Feature Engineering for Revenue

```python
# Add revenue-specific features
# (Requires updating feature_engineering.py)

# Price sensitivity indicators
- High engagement + no purchase = price-sensitive?
- Device type (mobile users spend less?)
```

### 2. Residual Analysis

```python
import matplotlib.pyplot as plt

# Check prediction errors
residuals = y_test - y_pred

plt.scatter(y_pred, residuals)
plt.xlabel('Predicted Revenue')
plt.ylabel('Residuals')
plt.axhline(0, color='red', linestyle='--')
plt.title('Residual Plot')
plt.savefig('reports/figures/residuals.png')

# Look for patterns:
# - Random scatter = good
# - Funnel shape = heteroscedasticity
# - Curved pattern = non-linear relationship
```

### 3. Feature Importance

```python
# For Random Forest
importance = model.feature_importances_
feature_names = X_train.columns

# Sort and plot
indices = np.argsort(importance)[::-1]
plt.barh(range(len(importance)), importance[indices])
plt.yticks(range(len(importance)), [feature_names[i] for i in indices])
plt.xlabel('Importance')
plt.title('Feature Importance for Revenue Prediction')
plt.tight_layout()
plt.savefig('reports/figures/revenue_feature_importance.png')
```

---

## üì¶ Required Libraries

Already installed:
- `scikit-learn` - Random Forest, Linear Regression, etc.
- `xgboost` - XGBoost (if you use it)
- `mlflow` - Experiment tracking
- `pandas`, `numpy` - Data manipulation

Optional:
```bash
pip install lightgbm  # For LightGBM
```

---

## üìù Deliverables Checklist

Before you consider yourself "done":

- [ ] Implemented `train_model()` function
- [ ] Chosen regression strategy (`converting_only` recommended)
- [ ] Achieved RMSE < $25 (minimum)
- [ ] Tested locally: `python src/regression/train_model.py`
- [ ] Verified MLflow logging works
- [ ] Model saved to MinIO successfully
- [ ] Tested in Airflow DAG
- [ ] Documented your approach (add comments in code)
- [ ] Tried at least 2 different algorithms
- [ ] Performed hyperparameter tuning (optional but recommended)
- [ ] Analyzed feature importance
- [ ] Checked residual plots for patterns

---

## üöÄ Next Steps After Completion

Once your model is working:

1. **Coordinate with Classification Engineer**
   - If using two-stage approach, integrate both models
   - Test end-to-end prediction pipeline

2. **Error Analysis**
   - Which sessions have highest prediction error?
   - Are high-value orders harder to predict?
   - Do certain landing pages or traffic sources have worse predictions?

3. **Model Comparison**
   - Compare different strategies (converting_only vs. two-stage)
   - Compare algorithms (Random Forest vs. XGBoost)
   - Select best based on RMSE and R¬≤

---

## üìû Need Help?

1. **Data Questions**: Check [FEATURE_ANALYSIS_STRATEGY.md](file:///Users/jul/Desktop/uni/customer-churn-mlops/FEATURE_ANALYSIS_STRATEGY.md)
2. **Infrastructure Issues**: Contact Lead Engineer
3. **Zero-Inflation Strategy**: See detailed discussion in [FEATURE_ANALYSIS_STRATEGY.md](file:///Users/jul/Desktop/uni/customer-churn-mlops/FEATURE_ANALYSIS_STRATEGY.md) (Section: Regression Model)
4. **Algorithm Help**: Check [ML_ENGINEER_HANDOFF.md](file:///Users/jul/Desktop/uni/customer-churn-mlops/ML_ENGINEER_HANDOFF.md)

---

## üìö Resources

**Algorithms**:
- [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
- [XGBoost Regressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)
- [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)

**Metrics**:
- [RMSE vs MAE](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)
- [R¬≤ Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)

**Zero-Inflation**:
- [Two-Part Models](https://stats.stackexchange.com/questions/38906/predicting-revenue-with-zero-inflated-data)
- [Log Transform](https://scikit-learn.org/stable/modules/preprocessing.html#mapping-to-a-gaussian-distribution)

---

**Good luck! Remember: Start with `converting_only` strategy, leverage engagement_depth, and aim for RMSE < $20! üöÄ**
