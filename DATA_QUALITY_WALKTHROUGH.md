# Data Quality Validation - Walkthrough

## Overview

Built a comprehensive **data quality validation system** that runs before feature engineering to ensure data integrity, monitor drift, and catch issues early.

## ğŸ—ï¸ What Was Built

### 1. Data Quality Validator Utility
**File**: [src/lead_ds/data_quality_validator.py](file:///Users/jul/Desktop/uni/customer-churn-mlops/src/lead_ds/data_quality_validator.py)

**Validation Checks**:

#### âœ… Null Value Detection
- Identifies columns with null values
- Critical columns (IDs, timestamps) trigger **ERRORS** if null
- Non-critical columns trigger **WARNINGS** if > 5% nulls
- Tracks null percentage and counts

#### âœ… Schema Validation  
- Compares actual schema to expected schema
- Checks for:
  - Missing required columns (ERROR)
  - Extra unexpected columns (WARNING)
  - Data type mismatches (WARNING)
- Pre-defined schemas for all data files

#### âœ… Data Range Checks
- Validates values are within expected ranges:
  - `price_usd`: Must be positive, < $10,000
  - `hour_of_day`: Must be 0-23
  - `engagement_depth`: Must be non-negative
- Invalid ranges trigger **ERRORS**

#### âœ… Data Drift Monitoring
- Compares current data statistics to baseline
- Tracks mean, std, min, max, median for numeric columns
- Alerts if > 20% change detected (WARNING)
- Creates baseline on first run

---

### 2. Airflow DAG for Orchestration
**File**: [dags/data_quality_dag.py](file:///Users/jul/Desktop/uni/customer-churn-mlops/dags/data_quality_dag.py)

**Two DAG Variants**:

#### DAG 1: `data_quality_validation` (Scheduled)
Runs **daily** and integrates with feature engineering pipeline.

**Pipeline Flow**:
```
check_data_freshness
         |
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼        â–¼
validate_   validate_  validate_
 sessions    orders    pageviews
    |         |        |
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
wait_for_feature_engineering
         â–¼
validate_processed_data
         â–¼
generate_summary_report
```

**Features**:
- âœ… Validates raw data freshness (alerts if > 30 days old)
- âœ… Validates all 3 raw data files in parallel
- âœ… Waits for feature engineering to complete
- âœ… Validates processed training data
- âœ… Generates summary report

#### DAG 2: `data_quality_validation_standalone` (Manual)
For quick validation without waiting for feature engineering.

**Pipeline Flow**:
```
check_data_freshness
         |
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼        â–¼
validate_   validate_  validate_
 sessions    orders    pageviews
    |         |        |
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
generate_summary_report
```

---

## ğŸ§ª Testing Results

### Test 1: Validation on Training Data

```bash
$ python src/lead_ds/data_quality_validator.py
```

**Output**:
```
============================================================
  DATA QUALITY VALIDATION
  File: processed-data/training_data.parquet
============================================================
ğŸ“¥ Loading training_data.parquet from processed-data...
âœ… Loaded 472,871 rows, 14 columns

ğŸ” Checking for null values...
   âœ… No null values found

ğŸ” Validating schema...
   âš ï¸  Schema issues found

ğŸ” Checking data ranges...
   âœ… All values within expected ranges

ğŸ” Checking for data drift...
   âœ… Baseline statistics saved to data_quality/baseline_stats.json

============================================================
âœ… DATA QUALITY VALIDATION PASSED

ğŸ“Š Summary:
   Errors: 0
   Warnings: 2

âš ï¸  Warnings:
   - Unexpected columns (one-hot encoded features)
   - Data type mismatch: hour_of_day (int32 vs int64)
============================================================
```

**Validation Results**:
- âœ… **No null values** in 472,871 rows
- âœ… **No range violations** (all values valid)
- âœ… **Baseline created** for drift monitoring
- âš ï¸ **2 warnings** (non-critical):
  - Extra one-hot encoded columns (expected after encoding)
  - Minor dtype difference (int32 vs int64, both valid)

**Validation Report**: `reports/data_quality/validation_report_20260207_142512.json`

---

## ğŸ“Š Validation Report Structure

**JSON Report Format**:
```json
{
  "timestamp": "2026-02-07T14:25:12",
  "file": "processed-data/training_data.parquet",
  "passed": true,
  "errors": [],
  "warnings": ["..."],
  "checks": {
    "null_values": {
      "total_nulls": 0,
      "columns_with_nulls": [],
      "null_summary": {}
    },
    "schema": {
      "expected_columns": 6,
      "actual_columns": 14,
      "missing_columns": [],
      "extra_columns": ["landing_page_/lander-1", ...],
      "dtype_mismatches": []
    },
    "data_ranges": {
      "issues_found": 0,
      "issues": []
    },
    "data_drift": {
      "baseline_exists": false,
      "drift_detected": 0,
      "current_stats": {...}
    }
  }
}
```

---

## ğŸ”§ Configuration

### Validation Thresholds

| Check | Threshold | Action |
|-------|-----------|--------|
| Null values in critical columns | > 0 | ERROR - Stop pipeline |
| Null values in non-critical columns | > 5% | WARNING |
| Negative prices | > 0 | ERROR |
| Invalid hour (not 0-23) | > 0 | ERROR |
| Data freshness | > 30 days | WARNING |
| Data drift | > 20% change | WARNING |

### Critical Columns

**Raw Data**:
- `website_session_id` (must not be null)
- `created_at` (must not be null)
- `user_id` (must not be null)

**Processed Data**:
- `is_ordered` (target)
- `revenue` (target)

---

## ğŸ“ Output Files

### Validation Reports
**Location**: `reports/data_quality/`

**Files Generated**:
- `validation_report_YYYYMMDD_HHMMSS.json` - Individual validation report
- `quality_summary.json` - Summary of latest validations

### Baseline Statistics
**Location**: `data_quality/baseline_stats.json`

**Purpose**: Reference for drift detection

**Example**:
```json
{
  "hour_of_day": {
    "mean": 12.5,
    "std": 6.9,
    "min": 0,
    "max": 23,
    "median": 12
  },
  "engagement_depth": {
    "mean": 2.5,
    "std": 1.8,
    "min": 0,
    "max": 50,
    "median": 2
  }
}
```

---

## ğŸ¯ How to Use

### Run Validation Manually (Local)

```bash
# Validate processed data
python src/lead_ds/data_quality_validator.py
```

**To validate different files**, modify the script:
```python
# Validate raw sessions
validator = DataQualityValidator("raw-data", "website_sessions.parquet")

# Validate orders
validator = DataQualityValidator("raw-data", "orders.parquet")

# Validate pageviews
validator = DataQualityValidator("raw-data", "website_pageviews.parquet")
```

### Run Validation via Airflow

**Option 1: Scheduled Daily Validation**
```bash
# Already scheduled to run daily
# Check Airflow UI at http://localhost:8080
# DAG: data_quality_validation
```

**Option 2: Manual Quick Check**
```bash
# Trigger standalone validation (no feature engineering wait)
airflow dags trigger data_quality_validation_standalone
```

### Check Validation Results

**Via Reports**:
```bash
# View latest validation report
cat reports/data_quality/quality_summary.json

# View specific report
cat reports/data_quality/validation_report_20260207_142512.json
```

**Via Airflow Logs**:
- Navigate to Airflow UI
- Select `data_quality_validation` DAG
- Click on task (e.g., `validate_raw_sessions`)
- View logs

---

## ğŸš¨ What Happens When Validation Fails?

### ERROR Conditions (Pipeline Stops)
If validation encounters **ERRORS**:
1. Task fails in Airflow
2. Downstream tasks (feature engineering, model training) **do NOT run**
3. Alert sent (if configured)
4. Report saved with failure details

**Common ERROR scenarios**:
- Critical columns have null values
- Negative prices detected
- Invalid hour values (not 0-23)
- Data file missing or corrupted

### WARNING Conditions (Pipeline Continues)
If validation encounters **WARNINGS**:
1. Task succeeds in Airflow
2. Pipeline continues normally
3. Warnings logged in report

**Common WARNING scenarios**:
- Non-critical columns have some nulls
- Extra columns detected (e.g., after one-hot encoding)
- Minor data type differences (int32 vs int64)
- Data drift detected (> 20% change from baseline)

---

## ğŸ“ˆ Integration with Pipeline

### Current Pipeline Flow

```
1. Maven Ingestion DAG
   â””â”€â”€ Uploads CSV â†’ Parquet to MinIO
         â”‚
         â–¼
2. Data Quality Validation DAG (NEW!)
   â””â”€â”€ Validates raw data
         â”‚
         â–¼ (only if validation passes)
3. Feature Engineering DAG
   â””â”€â”€ Creates training_data.parquet
         â”‚
         â–¼
4. Data Quality Validation (again)
   â””â”€â”€ Validates processed data
         â”‚
         â–¼ (only if validation passes)
5. Model Training DAG
   â””â”€â”€ Trains classification & regression models
```

**Benefits**:
- âœ… Catches bad data **before** expensive feature engineering
- âœ… Monitors data drift over time
- âœ… Automated daily checks
- âœ… Historical tracking via reports

---

## ğŸ”¬ Advanced Features

### 1. Baseline Management

**First Run**: Creates baseline automatically
```
   âœ… Baseline statistics saved to data_quality/baseline_stats.json
```

**Subsequent Runs**: Compares to baseline
```
   âš ï¸  Drift detected in engagement_depth.mean: 25.3% change
```

**Update Baseline**: If drift is expected (e.g., seasonality):
```bash
# Delete old baseline to create new one
rm data_quality/baseline_stats.json

# Re-run validation
python src/lead_ds/data_quality_validator.py
```

### 2. Custom Validation Rules

**Add new checks** by extending `DataQualityValidator`:
```python
def check_business_rules(self, df):
    """Custom business logic validation"""
    # Example: Check conversion rate is reasonable
    conversion_rate = df['is_ordered'].mean()
    if conversion_rate < 0.01 or conversion_rate > 0.50:
        self.validation_results['warnings'].append(
            f"Unusual conversion rate: {conversion_rate*100:.2f}%"
        )
```

### 3. Alert Integration

**Add email alerts** when validation fails:
```python
from airflow.operators.email import EmailOperator

send_alert = EmailOperator(
    task_id='send_validation_alert',
    to='data-team@company.com',
    subject='Data Quality Validation Failed',
    html_content='...',
    trigger_rule='one_failed'
)
```

---

## ğŸ“Š Monitoring Dashboard Ideas

### Suggested Metrics to Track
1. **Validation Pass Rate**: % of validations that pass
2. **Null Rate Trend**: Track null % over time
3. **Drift Magnitude**: How much features drift
4. **Data Freshness**: Age of latest data
5. **Validation Runtime**: Time to complete checks

### Example Dashboard Query
```python
# Aggregate all validation reports
import json, glob

reports = []
for file in glob.glob('reports/data_quality/validation_report_*.json'):
    with open(file) as f:
        reports.append(json.load(f))

# Calculate metrics
pass_rate = sum(1 for r in reports if r['passed']) / len(reports) * 100
avg_warnings = sum(len(r['warnings']) for r in reports) / len(reports)

print(f"Pass Rate: {pass_rate:.1f}%")
print(f"Avg Warnings: {avg_warnings:.1f}")
```

---

## ğŸ“ Best Practices

### 1. Run Validation Before Expensive Operations
âœ… **Do**: Validate raw data before feature engineering  
âŒ **Don't**: Train models on unvalidated data

### 2. Update Baselines Periodically
- Review baselines quarterly or when business changes
- Don't blindly auto-update (drift might indicate real issues)

### 3. Monitor Trends, Not Just Snapshots
- Track validation metrics over time
- Look for patterns (e.g., data quality degrades on weekends)

### 4. Document Expected Warnings
- Some warnings are expected (e.g., one-hot encoding creates extra columns)
- Document these in code comments

### 5. Set Up Alerts for Critical Failures
- Email/Slack alerts when ERROR conditions occur
- Route to on-call engineer

---

## Summary

âœ… **Built**:
- Comprehensive validation utility with 4 check types
- Two Airflow DAGs (scheduled + standalone)
- JSON reporting system
- Baseline drift detection

âœ… **Tested**:
- Validated 472,871 rows successfully
- 0 errors, 2 expected warnings
- Baseline created for future drift monitoring

âœ… **Integrated**:
- Runs before feature engineering
- Validates both raw and processed data
- Daily scheduled checks

**Your data pipeline now has automated quality gates to catch issues early! ğŸ¯**
